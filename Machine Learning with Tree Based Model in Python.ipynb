{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260ad73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Train your first classification tree\n",
    "\n",
    "#In this exercise you'll work with the Wisconsin Breast \n",
    "#Cancer Dataset from the UCI machine learning repository.\n",
    "# You'll predict whether a tumor is malignant or benign \n",
    "#based on two features: the mean radius of the tumor \n",
    "#(radius_mean) and its mean number of concave points \n",
    "#(concave points_mean).\n",
    "\n",
    "#The dataset is already loaded in your workspace and is \n",
    "#split into 80% train and 20% test. The feature matrices are \n",
    "#assigned to X_train and X_test, while the arrays of \n",
    "#labels are assigned to y_train and y_test where class 1 \n",
    "#corresponds to a malignant tumor and class 0 corresponds \n",
    "#to a benign tumor. To obtain reproducible results, we also \n",
    "#defined a variable called SEED which is set to 1.\n",
    "\n",
    "#Import DecisionTreeClassifier from sklearn.tree.\n",
    "\n",
    "#Instantiate a DecisionTreeClassifier dt of maximum \n",
    "#depth equal to 6.\n",
    "\n",
    "#Fit dt to the training set.\n",
    "\n",
    "#Predict the test set labels and assign the result to \n",
    "#y_pred.\n",
    "\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('wbc.csv')\n",
    "X = df[['radius_mean', 'concave points_mean']]\n",
    "y = df['diagnosis']\n",
    "y = y.map({'M':1, 'B':0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad8a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the classification tree\n",
    "\n",
    "#Now that you've fit your first classification tree, it's time to \n",
    "#evaluate its performance on the test set. You'll do so using \n",
    "#the accuracy metric which corresponds to the fraction of \n",
    "#correct predictions made on the test set.\n",
    "\n",
    "#The trained model dt from the previous exercise is loaded \n",
    "#in your workspace along with the test set features matrix \n",
    "#X_test and the array of labels y_test.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the function accuracy_score from \n",
    "#sklearn.metrics.\n",
    "\n",
    "#Predict the test set labels and assign the obtained array \n",
    "#to y_pred.\n",
    "\n",
    "#Evaluate the test set accuracy score of dt by calling \n",
    "#accuracy_score() and assign the value to acc.\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b56709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis hernandez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\luis hernandez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Logistic regression vs classification tree\n",
    "\n",
    "#A classification tree divides the feature space into \n",
    "#rectangular regions. In contrast, a linear model such as \n",
    "#logistic regression produces only a single linear decision \n",
    "#boundary dividing the feature space into \n",
    "#two decision regions.\n",
    "\n",
    "#We have written a custom function called \n",
    "#plot_labeled_decision_regions() that you can use to \n",
    "#plot the decision regions of a list containing two trained \n",
    "#classifiers. You can type \n",
    "#help(plot_labeled_decision_regions) in the IPython \n",
    "#shell to learn more about this function.\n",
    "\n",
    "#X_train, X_test, y_train, y_test, the model dt that \n",
    "#you've trained in an earlier exercise , as well as the \n",
    "#function plot_labeled_decision_regions() are available \n",
    "#in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import LogisticRegression from \n",
    "#sklearn.linear_model.\n",
    "\n",
    "#Instantiate a LogisticRegression model and assign it to\n",
    "# logreg.\n",
    "\n",
    "#Fit logreg to the training set.\n",
    "\n",
    "#Review the plot generated by \n",
    "#plot_labeled_decision_regions().\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "\n",
    "def plot_labeled_decision_regions(X,y, models):\n",
    "    '''Function producing a scatter plot of the instances contained \n",
    "    in the 2D dataset (X,y) along with the decision \n",
    "    regions of two trained classification models contained in the\n",
    "    list 'models'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas DataFrame corresponding to two numerical features \n",
    "    y: pandas Series corresponding the class labels\n",
    "    models: list containing two trained classifiers \n",
    "    \n",
    "    '''\n",
    "    if len(models) != 2:\n",
    "        raise Exception('''Models should be a list containing only two trained classifiers.''')\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise Exception('''X has to be a pandas DataFrame with two numerical features.''')\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise Exception('''y has to be a pandas Series corresponding to the labels.''')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)\n",
    "    for i, model in enumerate(models):\n",
    "        plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])\n",
    "        ax[i].set_title(model.__class__.__name__)\n",
    "        ax[i].set_xlabel(X.columns[0])\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(X.columns[1])\n",
    "            ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "            ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "plot_labeled_decision_regions(X_test, y_test, clfs)\n",
    "\n",
    "#Great work! Notice how the decision boundary produced \n",
    "#by logistic regression is linear while the boundaries produced \n",
    "#by the classification tree divide the feature space into rectangular\n",
    " #regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e946271e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using entropy as a criterion\n",
    "\n",
    "#In this exercise, you'll train a classification tree on the \n",
    "#Wisconsin Breast Cancer dataset using entropy as an \n",
    "#information criterion. You'll do so using all the 30 features \n",
    "#in the dataset, which is split into 80% train and 20% test.\n",
    "\n",
    "#X_train as well as the array of labels y_train are \n",
    "#available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import DecisionTreeClassifier from sklearn.tree.\n",
    "\n",
    "#Instantiate a DecisionTreeClassifier dt_entropy with a maximum depth of 8.\n",
    "\n",
    "#Set the information criterion to 'entropy'.\n",
    "\n",
    "#Fit dt_entropy on the training set.\n",
    "\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b30a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.8947368421052632\n",
      "Accuracy achieved by using the gini index:  0.8859649122807017\n"
     ]
    }
   ],
   "source": [
    "#Entropy vs Gini index\n",
    "\n",
    "#In this exercise you'll compare the test set accuracy of \n",
    "#dt_entropy to the accuracy of another tree named \n",
    "#dt_gini. The tree dt_gini was trained on the same \n",
    "#dataset using the same parameters except for the \n",
    "#information criterion which was set to the gini index using \n",
    "#the keyword 'gini'.\n",
    "\n",
    "#X_test, y_test, dt_entropy, as well as accuracy_gini \n",
    "#which corresponds to the test set accuracy achieved by \n",
    "#dt_gini are available in your workspace.\n",
    "\n",
    "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import accuracy_score from sklearn.metrics.\n",
    "\n",
    "#Predict the test set labels of dt_entropy and assign the \n",
    "#result to y_pred.\n",
    "\n",
    "#Evaluate the test set accuracy of dt_entropy and assign \n",
    "#the result to accuracy_entropy.\n",
    "\n",
    "#Review accuracy_entropy and accuracy_gini.\n",
    "\n",
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "y_pred_gini = dt_gini.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
    "\n",
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)\n",
    "\n",
    "\n",
    "#Nice work! Notice how the two models achieve exactly \n",
    "#the same accuracy. Most of the time, the gini index and \n",
    "#entropy lead to the same results. The gini index is slightly \n",
    "#faster to compute and is the default criterion used in the \n",
    "#DecisionTreeClassifier model of scikit-learn-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31910413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train your first regression tree\n",
    "\n",
    "#In this exercise, you'll train a regression tree to predict the \n",
    "#mpg (miles per gallon) consumption of cars in the auto-\n",
    "#mpg dataset using all the six available features.\n",
    "\n",
    "#The dataset is processed for you and is split to 80% train \n",
    "#and 20% test. The features matrix X_train and the array \n",
    "#y_train are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import DecisionTreeRegressor from sklearn.tree.\n",
    "\n",
    "#Instantiate a DecisionTreeRegressor dt with maximum \n",
    "#depth 8 and min_samples_leaf set to 0.13.\n",
    "\n",
    "#Fit dt to the training set.\n",
    "\n",
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aec5e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 0.27\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the regression tree\n",
    "\n",
    "#In this exercise, you will evaluate the test set performance \n",
    "#of dt using the Root Mean Squared Error (RMSE) metric.\n",
    "# The RMSE of a model measures, on average, how much the \n",
    "#model's predictions differ from the actual labels. The RMSE \n",
    "#of a model can be obtained by computing the square root\n",
    " #of the model's Mean Squared Error (MSE).\n",
    "\n",
    "#The features matrix X_test, the array y_test, as well as \n",
    "#the decision tree regressor dt that you trained in the \n",
    "#previous exercise are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the function mean_squared_error as MSE from \n",
    "#sklearn.metrics.\n",
    "\n",
    "#Predict the test set labels and assign the output to \n",
    "#y_pred.\n",
    "\n",
    "#Compute the test set MSE by calling MSE and assign the \n",
    "#result to mse_dt.\n",
    "\n",
    "#Compute the test set RMSE and assign it to rmse_dt.\n",
    "\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**0.5\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df2a40dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 0.31\n",
      "Regression Tree test set RMSE: 0.27\n"
     ]
    }
   ],
   "source": [
    "#Linear regression vs regression tree\n",
    "\n",
    "#In this exercise, you'll compare the test set RMSE of dt to \n",
    "#that achieved by a linear regression model. We have \n",
    "#already instantiated a linear regression model lr and \n",
    "#trained it on the same dataset as dt.\n",
    "\n",
    "#The features matrix X_test, the array of labels y_test, \n",
    "#the trained linear regression model lr,\n",
    " #mean_squared_error function which was imported under\n",
    " #the alias MSE and rmse_dt from the previous exercise are\n",
    " #available in your workspace.\n",
    "    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Predict test set labels using the linear regression model \n",
    "#(lr) and assign the result to y_pred_lr.\n",
    "\n",
    "#Compute the test set MSE and assign the result to mse_lr.\n",
    "\n",
    "#Compute the test set RMSE and assign the result to rmse_lr.\n",
    "\n",
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**0.5\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e7c07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the bagging classifier\n",
    "\n",
    "#In the following exercises you'll work with the Indian Liver \n",
    "#Patient dataset from the UCI machine learning repository. \n",
    "#Your task is to predict whether a patient suffers from a \n",
    "#liver disease using 10 features including Albumin, age and \n",
    "#gender. You'll do so using a Bagging Classifier.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import DecisionTreeClassifier from sklearn.tree \n",
    "#and BaggingClassifier from sklearn.ensemble.\n",
    "\n",
    "#Instantiate a DecisionTreeClassifier called dt.\n",
    "\n",
    "#Instantiate a BaggingClassifier called bc consisting of \n",
    "#50 trees.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb2b4125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.90\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Bagging performance\n",
    "\n",
    "#Now that you instantiated the bagging classifier, it's time \n",
    "#to train it and evaluate its test set accuracy.\n",
    "\n",
    "#The Indian Liver Patient dataset is processed for you and \n",
    "#split into 80% train and 20% test. The feature matrices \n",
    "#X_train and X_test, as well as the arrays of labels \n",
    "#y_train and y_test are available in your workspace. In \n",
    "#addition, we have also loaded the bagging classifier bc \n",
    "#that you instantiated in the previous exercise and the \n",
    "#function accuracy_score() from sklearn.metrics.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Fit bc to the training set.\n",
    "\n",
    "#Predict the test set labels and assign the result to y_pred.\n",
    "\n",
    "#Determine bc's test set accuracy.\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_pred, y_test)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e1eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the ground\n",
    "\n",
    "#In the following exercises, you'll compare the OOB \n",
    "#accuracy to the test set accuracy of a bagging classifier \n",
    "#trained on the Indian Liver Patient dataset.\n",
    "\n",
    "#In sklearn, you can evaluate the OOB accuracy of an \n",
    "#ensemble classifier by setting the parameter oob_score \n",
    "#to True during instantiation. After training the classifier, \n",
    "#the OOB accuracy can be obtained by accessing the\n",
    "# .oob_score_ attribute from the corresponding instance.\n",
    "\n",
    "#In your environment, we have made available the class \n",
    "#DecisionTreeClassifier from sklearn.tree.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import BaggingClassifier from sklearn.ensemble.\n",
    "\n",
    "#Instantiate a DecisionTreeClassifier with \n",
    "#min_samples_leaf set to 8.\n",
    "\n",
    "#Instantiate a BaggingClassifier consisting of 50 \n",
    "#trees and set oob_score to True.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8404a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.930, OOB accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "#OOB Score vs Test Set Score\n",
    "\n",
    "#Now that you instantiated bc, you will fit it to the training \n",
    "#set and evaluate its test set and OOB accuracies.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train \n",
    "#and 20% test. The feature matrices X_train and X_test,\n",
    " #as well as the arrays of labels y_train and y_test are\n",
    " #available in your workspace. In addition, we have also\n",
    " #loaded the classifier bc instantiated in the previous\n",
    " #exercise and the function accuracy_score() from\n",
    " #sklearn.metrics.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Fit bc to the training set and predict the test set labels\n",
    " #and assign the results to y_pred.\n",
    "\n",
    "#Evaluate the test set accuracy acc_test by calling\n",
    " #accuracy_score.\n",
    "\n",
    "#Evaluate bc's OOB accuracy acc_oob by extracting the\n",
    " #attribute oob_score_ from bc.\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f75b7a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=25, random_state=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train an RF regressor\n",
    "\n",
    "#In the following exercises you'll predict bike rental demand \n",
    "#in the Capital Bikeshare program in Washington, D.C using\n",
    " #historical weather data from the Bike Sharing Demand\n",
    " #dataset available through Kaggle. For this purpose, you will\n",
    " #be using the random forests algorithm. As a first step, you'll\n",
    " #define a random forests regressor and fit it to the training\n",
    " #set.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train\n",
    " #and 20% test. The features matrix X_train and the array\n",
    " #y_train are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import RandomForestRegressor from sklearn.ensemble.\n",
    "\n",
    "#Instantiate a RandomForestRegressor called rf \n",
    "#consisting of 25 trees.\n",
    "\n",
    "#Fit rf to the training set.\n",
    "\n",
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "154cdc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 0.27\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the RF regressor\n",
    "\n",
    "#You'll now evaluate the test set RMSE of the random\n",
    " #forests regressor rf that you trained in the previous\n",
    " #exercise.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train\n",
    " #and 20% test. The features matrix X_test, as well as the\n",
    " #array y_test are available in your workspace. In addition,\n",
    " #we have also loaded the model rf that you trained in the\n",
    " #previous exercise.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import mean_squared_error from sklearn.metrics as\n",
    " #MSE.\n",
    "\n",
    "#Predict the test set labels and assign the result to\n",
    " #y_pred.\n",
    "\n",
    "#Compute the test set RMSE and assign it to rmse_test\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test,y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84d20c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing features importances\n",
    "\n",
    "#In this exercise, you'll determine which features were the\n",
    " #most predictive according to the random forests regressor\n",
    " #rf that you trained in a previous exercise.\n",
    "\n",
    "#For this purpose, you'll draw a horizontal barplot of the\n",
    " #feature importance as assessed by rf. Fortunately, this\n",
    " #can be done easily thanks to plotting capabilities of\n",
    " #pandas.\n",
    "\n",
    "#We have created a pandas.Series object called\n",
    " #importances containing the feature names as index and\n",
    " #their importances as values. In addition,\n",
    " #matplotlib.pyplot is available as plt and pandas as\n",
    " #pd.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Call the .sort_values() method on importances and assign the result to importances_sorted.\n",
    "\n",
    "#Call the .plot() method on importances_sorted and set the arguments:\n",
    "\n",
    "#kind to 'barh'\n",
    "#color to 'lightgreen'\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a17f34e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DecisionTreeClassifier.__init__() got an unexpected keyword argument 'refit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_7804/266995095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Instantiate dt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Instantiate ada\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: DecisionTreeClassifier.__init__() got an unexpected keyword argument 'refit'"
     ]
    }
   ],
   "source": [
    "#Define the AdaBoost classifier\n",
    "\n",
    "#In the following exercises you'll revisit the Indian Liver \n",
    "#Patient dataset which was introduced in a previous\n",
    " #chapter. Your task is to predict whether a patient suffers\n",
    " #from a liver disease using 10 features including Albumin,\n",
    " #age and gender. However, this time, you'll be training an\n",
    " #AdaBoost ensemble to perform the classification task. In\n",
    " #addition, given that this dataset is imbalanced, you'll be\n",
    " #using the ROC AUC score as a metric instead of accuracy.\n",
    "\n",
    "#As a first step, you'll start by instantiating an AdaBoost\n",
    " #classifier.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import AdaBoostClassifier from sklearn.ensemble.\n",
    "\n",
    "#Instantiate a DecisionTreeClassifier with max_depth\n",
    " #set to 2.\n",
    "\n",
    "#Instantiate an AdaBoostClassifier consisting of 180\n",
    " #trees and setting the base_estimator to dt.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, refit=True, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f374389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the AdaBoost classifier\n",
    "\n",
    "#Now that you've instantiated the AdaBoost classifier ada,\n",
    " #it's time train it. You will also predict the probabilities of\n",
    " #obtaining the positive class in the test set. This can be\n",
    " #done as follows:\n",
    "\n",
    "#Once the classifier ada is trained, call the\n",
    " #.predict_proba() method by passing X_test as a\n",
    " #parameter and extract these probabilities by slicing all the\n",
    " #values in the second column as follows:\n",
    "\n",
    "#ada.predict_proba(X_test)[:,1]\n",
    "\n",
    "#The Indian Liver dataset is processed for you and split into\n",
    " #80% train and 20% test. Feature matrices X_train and\n",
    " #X_test, as well as the arrays of labels y_train and\n",
    " #y_test are available in your workspace. In addition, we\n",
    " #have also loaded the instantiated model ada from the\n",
    " #previous exercise.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Fit ada to the training set.\n",
    "\n",
    "#Evaluate the probabilities of obtaining the positive class\n",
    " #in the test set.\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train,y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fb71bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.96\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the AdaBoost classifier\n",
    "\n",
    "#Now that you're done training ada and predicting the\n",
    " #probabilities of obtaining the positive class in the test set,\n",
    " #it's time to evaluate ada's ROC AUC score. Recall that the\n",
    " #ROC AUC score of a binary classifier can be determined\n",
    " #using the roc_auc_score() function from sklearn.metrics.\n",
    "\n",
    "#The arrays y_test and y_pred_proba that you computed\n",
    " #in the previous exercise are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import roc_auc_score from sklearn.metrics.\n",
    "\n",
    "#Compute ada's test set ROC AUC score, assign it to\n",
    " #ada_roc_auc, and print it out.\n",
    "\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d5c9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the GB regressor\n",
    "\n",
    "#You'll now revisit the Bike Sharing Demand dataset that\n",
    " #was introduced in the previous chapter. Recall that your\n",
    " #task is to predict the bike rental demand using historical\n",
    " #weather data from the Capital Bikeshare program in\n",
    " #Washington, D.C.. For this purpose, you'll be using a\n",
    " #gradient boosting regressor.\n",
    "\n",
    "#As a first step, you'll start by instantiating a gradient\n",
    " #boosting regressor which you will train in the next exercise.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import GradientBoostingRegressor from\n",
    " #sklearn.ensemble.\n",
    "\n",
    "#Instantiate a gradient boosting regressor by setting the\n",
    " #parameters:\n",
    "\n",
    "#max_depth to 4\n",
    "\n",
    "#n_estimators to 200\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75b57045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the GB regressor\n",
    "\n",
    "#You'll now train the gradient boosting regressor gb that\n",
    " #you instantiated in the previous exercise and predict test\n",
    " #set labels.\n",
    "\n",
    "#The dataset is split into 80% train and 20% test. Feature\n",
    " #matrices X_train and X_test, as well as the arrays\n",
    " #y_train and y_test are available in your workspace. In\n",
    " #addition, we have also loaded the model instance gb that\n",
    " #you defined in the previous exercise.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Fit gb to the training set.\n",
    "\n",
    "#Predict the test set labels and assign the result to\n",
    " #y_pred.\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da91d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 0.301\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the GB regressor\n",
    "\n",
    "#Now that the test set predictions are available, you can \n",
    "#use them to evaluate the test set Root Mean Squared Error\n",
    " #(RMSE) of gb.\n",
    "\n",
    "#y_test and predictions y_pred are available in your\n",
    " #workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import mean_squared_error from sklearn.metrics \n",
    "#as MSE.\n",
    "\n",
    "#Compute the test set MSE and assign it to mse_test.\n",
    "\n",
    "#Compute the test set RMSE and assign it to rmse_test.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test,y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**0.5\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77812fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression with SGB\n",
    "\n",
    "#As in the exercises from the previous lesson, you'll be\n",
    " #working with the Bike Sharing Demand dataset. In the\n",
    " #following set of exercises, you'll solve this bike count\n",
    " #regression problem using stochastic gradient boosting.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Instantiate a Stochastic Gradient Boosting Regressor\n",
    " #(SGBR) and set:\n",
    "\n",
    "#max_depth to 4 and n_estimators to 200,\n",
    "\n",
    "#subsample to 0.9, and\n",
    "\n",
    "#max_features to 0.75.\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee51dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SGB regressor\n",
    "\n",
    "#In this exercise, you'll train the SGBR sgbr instantiated in\n",
    " #the previous exercise and predict the test set labels.\n",
    "\n",
    "#The bike sharing demand dataset is already loaded\n",
    " #processed for you; it is split into 80% train and 20% test.\n",
    " #The feature matrices X_train and X_test, the arrays of\n",
    " #labels y_train and y_test, and the model instance\n",
    " #sgbr that you defined in the previous exercise are\n",
    " #available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Fit sgbr to the training set.\n",
    "\n",
    "#Predict the test set labels and assign the results to\n",
    "#y_pred.\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05fd8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 0.301\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the SGB regressor\n",
    "\n",
    "#You have prepared the ground to determine the test set \n",
    "#RMSE of sgbr which you shall evaluate in this exercise.\n",
    "\n",
    "#y_pred and y_test are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import mean_squared_error as MSE from\n",
    " #sklearn.metrics.\n",
    "\n",
    "#Compute test set MSE and assign the result to mse_test.\n",
    "\n",
    "#Compute test set RMSE and assign the result to\n",
    " #rmse_test.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test,y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5610de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the tree's hyperparameter grid\n",
    "\n",
    "#In this exercise, you'll manually set the grid of\n",
    " #hyperparameters that will be used to tune the \n",
    "#classification tree dt and find the optimal classifier in the \n",
    "#next exercise.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Define a grid of hyperparameters corresponding to a \n",
    "#Python dictionary called params_dt with:\n",
    "\n",
    "#the key 'max_depth' set to a list of values 2, 3, and 4\n",
    "\n",
    "#the key 'min_samples_leaf' set to a list of values \n",
    "#0.12, 0.14, 0.16, 0.18\n",
    "\n",
    "# Define params_dt\n",
    "params_dt = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'min_samples_leaf':[0.12,0.14,0.16,0.18]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05cef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for the optimal tree\n",
    "\n",
    "#In this exercise, you'll perform grid search using 5-fold \n",
    "#cross validation to find dt's optimal hyperparameters. \n",
    "#Note that because grid search is an exhaustive process, it \n",
    "#may take a lot time to train the model. Here you'll only be \n",
    "#instantiating the GridSearchCV object without fitting it to \n",
    "#the training set. As discussed in the video, you can train\n",
    "#such an object similar to any scikit-learn estimator by \n",
    "#using the .fit() method:\n",
    "\n",
    "#grid_object.fit(X_train, y_train)\n",
    "\n",
    "#An untuned classification tree dt as well as the dictionary\n",
    "#params_dt that you defined in the previous exercise are \n",
    "#available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import GridSearchCV from sklearn.model_selection.\n",
    "\n",
    "#Instantiate a GridSearchCV object using 5-fold CV by \n",
    "#setting the parameters:\n",
    "\n",
    "#estimator to dt, param_grid to params_dt and\n",
    "\n",
    "#scoring to 'roc_auc'.\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       refit=True,\n",
    "                       n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "473cb164",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_7804/2543280697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Extract the best estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Predict the test set probabilities of the positive class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "#Evaluate the optimal tree\n",
    "\n",
    "#In this exercise, you'll evaluate the test set ROC AUC score\n",
    " #of grid_dt's optimal model.\n",
    "\n",
    "#In order to do so, you will first determine the probability of \n",
    "#obtaining the positive label for each test set observation. \n",
    "#You can use the methodpredict_proba() of an sklearn \n",
    "#classifier to compute a 2D array containing the\n",
    " #probabilities of the negative and positive class-labels \n",
    "#respectively along columns.\n",
    "\n",
    "#The dataset is already loaded and processed for you \n",
    "#(numerical features are standardized); it is split into 80%\n",
    " #train and 20% test. X_test, y_test are available in your \n",
    "#workspace. In addition, we have also loaded the trained\n",
    " #GridSearchCV object grid_dt that you instantiated in \n",
    "#the previous exercise. Note that grid_dt was trained as \n",
    "#follows:\n",
    "\n",
    "#grid_dt.fit(X_train, y_train)\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import roc_auc_score from sklearn.metrics.\n",
    "\n",
    "#Extract the .best_estimator_ attribute from grid_dt \n",
    "#and assign it to best_model.\n",
    "\n",
    "#Predict the test set probabilities of obtaining the positive \n",
    "#class y_pred_proba.\n",
    "\n",
    "#Compute the test set ROC AUC score test_roc_auc of \n",
    "#best_model.\n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = grid_dt.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test,y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad5f7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the hyperparameter grid of RF\n",
    "\n",
    "#In this exercise, you'll manually set the grid of hyperparameters that will\n",
    "#be used to tune rf's hyperparameters and find the optimal regressor. For this purpose, \n",
    "#you will be constructing a grid of hyperparameters and tune the number of estimators, \n",
    "#the maximum number of features used when splitting each node and the minimum number of \n",
    "#samples (or fraction) per leaf.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Define a grid of hyperparameters corresponding to a \n",
    "#Python dictionary called params_rf with:\n",
    "\n",
    "#the key 'n_estimators' set to a list of values 100, 350, 500\n",
    "\n",
    "#the key 'max_features' set to a list of values 'log2', 'auto', 'sqrt'\n",
    "\n",
    "#the key 'min_samples_leaf' set to a list of values 2, 10, 30\n",
    "\n",
    "# Define the dictionary 'params_rf'\n",
    "params_rf = { \n",
    "    'n_estimators':[100,350,500],\n",
    "    'max_features':['log2','auto','sqrt'],\n",
    "    'min_samples_leaf':[2,10,30]\n",
    "\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4ea0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for the optimal forest\n",
    "\n",
    "#In this exercise, you'll perform grid search using 3-fold \n",
    "#cross validation to find rf's optimal hyperparameters. To \n",
    "#evaluate each model in the grid, you'll be using the \n",
    "#negative mean squared error metric.\n",
    "\n",
    "#Note that because grid search is an exhaustive search \n",
    "#process, it may take a lot time to train the model. Here \n",
    "#you'll only be instantiating the GridSearchCV object \n",
    "#without fitting it to the training set. As discussed in the \n",
    "#video, you can train such an object similar to any scikit-\n",
    "#learn estimator by using the .fit() method:\n",
    "\n",
    "#grid_object.fit(X_train, y_train)\n",
    "\n",
    "#The untuned random forests regressor model rf \n",
    "#as well as the dictionary params_rf that you defined in the \n",
    "#previous exercise are available in your workspace.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import GridSearchCV from sklearn.model_selection.\n",
    "\n",
    "#Instantiate a GridSearchCV object using 3-fold CV by \n",
    "#using negative mean squared error as the scoring metric.\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,refit=True,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09548cc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_7804/2306197760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Extract the best estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Predict test set labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "#Evaluate the optimal forest\n",
    "\n",
    "#In this last exercise of the course, you'll evaluate the test \n",
    "#set RMSE of grid_rf's optimal model.\n",
    "\n",
    "#The dataset is already loaded and processed for you and \n",
    "#is split into 80% train and 20% test. In your environment \n",
    "#are available X_test, y_test and the function \n",
    "#mean_squared_error from sklearn.metrics under the \n",
    "#alias MSE. In addition, we have also loaded the trained \n",
    "#GridSearchCV object grid_rf that you instantiated in \n",
    "#the previous exercise. Note that grid_rf was trained \n",
    "#as follows:\n",
    "\n",
    "#grid_rf.fit(X_train, y_train)\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import mean_squared_error as MSE from \n",
    "#sklearn.metrics.\n",
    "\n",
    "#Extract the best estimator from grid_rf and assign it to \n",
    "#best_model.\n",
    "\n",
    "#Predict best_model's test set labels and assign the result \n",
    "#to y_pred.\n",
    "\n",
    "#Compute best_model's test set RMSE.\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
